# lexical-and-semantic-comparison-of-transformers-for-paraphrasing
Este respositorio es parte de mi trabajo final de fin de master. 
This repository is part of my final master's thesis.

It shows the evaluation and comparison in tasks of paraphrasing models with different architectures.

The models used in the evaluation were:

- PEGASUS model available in the Hugging Face hub under the name of tuner007/pegasus_paraphrase
- T5-large model available on the Hugging Face hub as ramsrigouthamg/t5-large-paraphraser-diverse-high-quality
- BART model available at the Hugging Face hub under the eugenesiow/bart-paraphrase tag
- Own mT5 model
- Own DistilGPT2 model

Models were evaluated using a set of 5,000 phrases collected from the PAWS and TaPaCo datasets.

Firstly, the ability of the models to paraphrase correctly while maintaining the semantics and syntax in the sentences was evaluated.
This was done by using the 'paraphrase-xlm-r-multilingual-v1' model available in the Hugging phase. This is a model for generating sentence-enbeddings.
Sentence embeddings capture the semantic context of sentences and can be compared in vector space with measures such as cosine similarity.

For the evaluation of the lexical variability of the paraphrases generated by each model, the following measures were used:
- Jaccard
- BLEU-1
- BLEU-2
- BLEU-3
- ROUGE-L

The code and results for each evaluation are available in the notebook.
